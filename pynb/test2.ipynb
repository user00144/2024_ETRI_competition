{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8d1b17-ebcb-4def-974a-b14701806e59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T01:56:04.591080Z",
     "iopub.status.busy": "2024-06-24T01:56:04.590442Z",
     "iopub.status.idle": "2024-06-24T01:56:04.617009Z",
     "shell.execute_reply": "2024-06-24T01:56:04.616443Z",
     "shell.execute_reply.started": "2024-06-24T01:56:04.591036Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = emb_size ** -0.5\n",
    "        # self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "        self.key = nn.Linear(emb_size, emb_size, bias=False)\n",
    "        self.value = nn.Linear(emb_size, emb_size, bias=False)\n",
    "        self.query = nn.Linear(emb_size, emb_size, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_out = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        k = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "        v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
    "        # k,v,q shape = (batch_size, num_heads, seq_len, d_head)\n",
    "\n",
    "        attn = torch.matmul(q, k) * self.scale\n",
    "        # attn shape (seq_len, seq_len)\n",
    "        attn = nn.functional.softmax(attn, dim=-1)\n",
    "\n",
    "        # import matplotlib.pyplot as plt\n",
    "        # plt.plot(x[0, :, 0].detach().cpu().numpy())\n",
    "        # plt.show()\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        # out.shape = (batch_size, num_heads, seq_len, d_head)\n",
    "        out = out.transpose(1, 2)\n",
    "        # out.shape == (batch_size, seq_len, num_heads, d_head)\n",
    "        out = out.reshape(batch_size, seq_len, -1)\n",
    "        # out.shape == (batch_size, seq_len, d_model)\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LearnablePositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=1024):\n",
    "        super(LearnablePositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Each position gets its own embedding\n",
    "        # Since indices are always 0 ... max_len, we don't have to do a look-up\n",
    "        self.pe = nn.Parameter(torch.empty(max_len, d_model))  # requires_grad automatically set to True\n",
    "        nn.init.uniform_(self.pe, -0.02, 0.02)\n",
    "\n",
    "        # distance = torch.matmul(self.pe, self.pe[10])\n",
    "        # import matplotlib.pyplot as plt\n",
    "\n",
    "        # plt.plot(distance.detach().numpy())\n",
    "        # plt.show()\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe\n",
    "        # distance = torch.matmul(self.pe, self.pe.transpose(1,0))\n",
    "        # distance_pd = pd.DataFrame(distance.cpu().detach().numpy())\n",
    "        # distance_pd.to_csv('learn_position_distance.csv')\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 bias=True):\n",
    "        super(CausalConv1d, self).__init__(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias)\n",
    "\n",
    "        self.__padding = (kernel_size - 1) * dilation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(CausalConv1d, self).forward(nn.functional.pad(x, (self.__padding, 0)))\n",
    "\n",
    "\n",
    "class CasualConvTran(nn.Module):\n",
    "    def __init__(self, seq_len, emb_size):\n",
    "        super().__init__()\n",
    "        channel_size= 3 #, 256\n",
    "        num_heads = 2\n",
    "        dim_ff = 512\n",
    "        \n",
    "        self.causal_Conv1 = nn.Sequential(CausalConv1d(channel_size, emb_size, kernel_size=8, stride=2, dilation=1),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        self.causal_Conv2 = nn.Sequential(CausalConv1d(emb_size, emb_size, kernel_size=5, stride=2, dilation=2),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        self.causal_Conv3 = nn.Sequential(CausalConv1d(emb_size, emb_size, kernel_size=3, stride=2, dilation=2),\n",
    "                                          nn.BatchNorm1d(emb_size), nn.GELU())\n",
    "\n",
    "        self.Fix_Position = LearnablePositionalEncoding(emb_size, dropout=0.1, max_len=seq_len)\n",
    "\n",
    "        self.attention_layer = Attention(emb_size, num_heads, 0.1)\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "        self.LayerNorm2 = nn.LayerNorm(emb_size, eps=1e-5)\n",
    "\n",
    "        self.FeedForward = nn.Sequential(\n",
    "            nn.Linear(emb_size, dim_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(dim_ff, emb_size),\n",
    "            nn.Dropout(0.1))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.unsqueeze(1)\n",
    "        x_src = self.causal_Conv1(x)\n",
    "        x_src = self.causal_Conv2(x_src)\n",
    "        x_src = self.causal_Conv3(x_src).squeeze(2)\n",
    "        x_src = x_src.permute(0, 2, 1)\n",
    "        x_src_pos = self.Fix_Position(x_src)\n",
    "        att = x_src + self.attention_layer(x_src_pos)\n",
    "        att = self.LayerNorm(att)\n",
    "        out = att + self.FeedForward(att)\n",
    "        out = self.LayerNorm2(out)\n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.gap(out)\n",
    "        out = self.flatten(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ace38f9-0a22-4393-8948-fc99f6ec8775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T02:10:45.282215Z",
     "iopub.status.busy": "2024-06-24T02:10:45.281597Z",
     "iopub.status.idle": "2024-06-24T02:10:45.315783Z",
     "shell.execute_reply": "2024-06-24T02:10:45.315263Z",
     "shell.execute_reply.started": "2024-06-24T02:10:45.282170Z"
    }
   },
   "outputs": [],
   "source": [
    "ConvTran = CasualConvTran(2653, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6e1ce84-be4f-4fda-a2e9-95916780ff6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T02:10:46.802338Z",
     "iopub.status.busy": "2024-06-24T02:10:46.801720Z",
     "iopub.status.idle": "2024-06-24T02:10:46.823711Z",
     "shell.execute_reply": "2024-06-24T02:10:46.823020Z",
     "shell.execute_reply.started": "2024-06-24T02:10:46.802293Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CasualConvTran                           [1, 512]                  --\n",
       "├─Sequential: 1-1                        [1, 512, 10610]           --\n",
       "│    └─CausalConv1d: 2-1                 [1, 512, 10610]           12,800\n",
       "│    └─BatchNorm1d: 2-2                  [1, 512, 10610]           1,024\n",
       "│    └─GELU: 2-3                         [1, 512, 10610]           --\n",
       "├─Sequential: 1-2                        [1, 512, 5305]            --\n",
       "│    └─CausalConv1d: 2-4                 [1, 512, 5305]            1,311,232\n",
       "│    └─BatchNorm1d: 2-5                  [1, 512, 5305]            1,024\n",
       "│    └─GELU: 2-6                         [1, 512, 5305]            --\n",
       "├─Sequential: 1-3                        [1, 512, 2653]            --\n",
       "│    └─CausalConv1d: 2-7                 [1, 512, 2653]            786,944\n",
       "│    └─BatchNorm1d: 2-8                  [1, 512, 2653]            1,024\n",
       "│    └─GELU: 2-9                         [1, 512, 2653]            --\n",
       "├─LearnablePositionalEncoding: 1-4       [1, 2653, 512]            1,358,336\n",
       "│    └─Dropout: 2-10                     [1, 2653, 512]            --\n",
       "├─Attention: 1-5                         [1, 2653, 512]            --\n",
       "│    └─Linear: 2-11                      [1, 2653, 512]            262,144\n",
       "│    └─Linear: 2-12                      [1, 2653, 512]            262,144\n",
       "│    └─Linear: 2-13                      [1, 2653, 512]            262,144\n",
       "│    └─LayerNorm: 2-14                   [1, 2653, 512]            1,024\n",
       "├─LayerNorm: 1-6                         [1, 2653, 512]            1,024\n",
       "├─Sequential: 1-7                        [1, 2653, 512]            --\n",
       "│    └─Linear: 2-15                      [1, 2653, 512]            262,656\n",
       "│    └─ReLU: 2-16                        [1, 2653, 512]            --\n",
       "│    └─Dropout: 2-17                     [1, 2653, 512]            --\n",
       "│    └─Linear: 2-18                      [1, 2653, 512]            262,656\n",
       "│    └─Dropout: 2-19                     [1, 2653, 512]            --\n",
       "├─LayerNorm: 1-8                         [1, 2653, 512]            1,024\n",
       "├─AdaptiveAvgPool1d: 1-9                 [1, 512, 1]               --\n",
       "├─Flatten: 1-10                          [1, 512]                  --\n",
       "==========================================================================================\n",
       "Total params: 4,787,200\n",
       "Trainable params: 4,787,200\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 9.18\n",
       "==========================================================================================\n",
       "Input size (MB): 0.25\n",
       "Forward/backward pass size (MB): 239.04\n",
       "Params size (MB): 13.72\n",
       "Estimated Total Size (MB): 253.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(ConvTran, input_size = (1, 3,21220))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
